{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b704982-203f-40a7-8479-d4fd5777c450",
   "metadata": {},
   "source": [
    "# Dataset Generator\n",
    "\n",
    "## Information\n",
    "- This notebook's main objective is to implement a dataset generator using a generative model. \n",
    "- This dataset generator can be used for any objective as long as the given prompt is clear enough.\n",
    "- This is just a tool, and is in no way perfect. You should *always* check the results by hand.\n",
    "- You should *always* generate a few examples before launching a full-scale generation.\n",
    "- This dataset generator uses *AzureOpenAI*, so make sure you have the right setup.\n",
    "- Should you wish to use your own API or model, the changes are minor, and the generator will adapt.\n",
    "- To exploit the generator's full potential, make sure to give a *clear* and *concise* prompt.\n",
    "- The model may take a longer time to generate the dataset.\n",
    "- The base output of the generator is a *yaml* file, though this can easily be modified.\n",
    "- The initial use for this generator is generating a dataset for La Roche Posay, so the examples given are relevant to that.\n",
    "\n",
    "- Version: 2.2\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f189d60-9a60-4fc3-b1ec-bed47c1daf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import asyncio\n",
    "from openai import OpenAI, AsyncOpenAI, AsyncAzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e134f78-7503-4e39-b9b5-25bf6ad5fedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the output is true\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4133bc-f955-49a8-a897-9e45d72f9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the client, you can load any client you'd like\n",
    "azure_client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "    api_version=\"2023-08-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57765a-dcf1-4acc-a76d-74f6fa37857e",
   "metadata": {},
   "source": [
    "## Generating functions\n",
    "\n",
    "### The heart of the mechanism\n",
    "\n",
    "This next cell contains the most important function of this notebook, **generate_question**. \n",
    "The creation of the dataset starts with this seemingly simple function, but its simplicity is also its greatest weakness. As we all know, artificial intelligence is just that, artificial and like any generative model, the performance will heavily depend on the prompt it's given. Not to put any pressure on you, dear reader, but the performance of this notebook is 90% on you.\n",
    "\n",
    "This function is the only function that will need heavy modifying in order for the generator to work properly (for you). The example given in this notebook is from a time I worked on generating a dataset for a skin care company, but you can do whatever you want with it. Take a look at my example, you can adapt it, modify it or even scrape it if it means you will get better performance. \n",
    "\n",
    "At the very end of this section there is a little test cell, make sure to **always** run it... run it as many times you need to until you get the structure you find satisfying. I'm heavily stressing the importance of these tests because once you launch the creation of your datasets, you're in it for a few hours. Make it count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef20f640-1c1c-4f3f-ab62-e497eb47cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific traits - relevant for the skin care products ###\n",
    "skin_types = [\"oily\", \"dry\"]\n",
    "product_types = [\"cleanser\", \"moisturizer\", \"serum\", \"eyecream\", \"cream\"]\n",
    "concerns = [\"simple_acne\", \"severe_acne\", \"dark_spots\", \"dryness\", \"dehydration\", \"fine_lines\", \n",
    "            \"allergy\", \"redness\", \"eczema\", \"wrinkles\", \"loss_of_firmness\", \"lack_of_elasticity\", \n",
    "            \"peeling_skin\", \"roughness\", \"tightness\", \"itchyness\", \"enlarged_pores\", \"excess_sebum\", \n",
    "            \"shining_zone\", \"hyperpigmentation\", \"dullness\", \"uneven_skin_tone\", \"skin_decoloration\", \n",
    "            \"dark_circles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f51c5855-1336-4c4d-b02a-00022f006a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate one singular question, adapt to your needs\n",
    "async def generate_question(id, azure_client=azure_client):\n",
    "\n",
    "    ### Custom part - modify to your liking ###\n",
    "    #skin_type = random.choice(skin_types)\n",
    "    #product_type = random.choice(product_types)\n",
    "    #product_line = random.choice(product_lines)\n",
    "    num_concerns = random.choice([1, 2])  # Using simple random choice for 1 or 2 concerns\n",
    "    selected_concerns = random.sample(concerns, num_concerns)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Generate a customer service question for a skincare product. You are human. You use La roche posay, but dont mention it by name all the time. \n",
    "        \n",
    "        You will generete a sentence based on {selected_concerns}.\n",
    "        You may mention the product type '{product_types}', the skin type '{skin_types}',\n",
    "        or some concerns from {selected_concerns}. \n",
    "        The examples you generate must always be related to the {selected_concerns}, otherwise the sentence won't make sense.\n",
    "        Be creative and provide diverse formulations.\n",
    "        Dont always use everything.``\n",
    "        Remember that you're human and you sometimes make mistakes when writing. \n",
    "        Make sure to include some questions just mentioning the issues, and not products or product lines,\n",
    "        or pretend you're asking for a generic product that could solve an issue.\n",
    "    \n",
    "        Examples: \n",
    "    \n",
    "        I have [dry skin](skintype). What would you suggest I do?\n",
    "        Do you have any [serum](producttype) that is good for [oily skin](skintype) and acne?\n",
    "    \n",
    "        When you find words from these groups:\n",
    "            - skin type: oily, dry\n",
    "            - product type: cleanser, moisturizer, serum, eyecream\n",
    "            - product line: cicaplast, effaclar, hyalu, lipikar, redermic, toleriane, pigmentclar\n",
    "        \n",
    "        always fromat them like in these examples:\n",
    "        \n",
    "        \\\"I'm looking for an [eyecream](producttype) that helps reduce dark circles. What do you suggest from [Pigmentclar](productline)?\\\"\n",
    "        \n",
    "        \\\"My [dry skin](skintype) feels very tight during winter. Could you recommend a [moisturizer](producttype)?\\\"\n",
    "        Always follow these rules:\n",
    "            Always format skin types, product types and product lines\n",
    "            Never format concerns!\n",
    "            Generate at most two full sentences, all in one line!\n",
    "            Always write full sentences.\n",
    "            Vary the length, I want some short sentences as well.\n",
    "            Human language isn't perfect, make sure your examples don't look too much like they were generated by an AI.\n",
    "            Rarely if never mention all four categories at the same time.\n",
    "            Always generate finish sentences.\n",
    "            Prefer making shorter sentences up to 30 tokens.\n",
    "            Occasionally put some spelling mistakes.\n",
    "            Never repeat the same example\n",
    "        Never break these rules!\n",
    "    \"\"\"\n",
    "    ### End of the custom part ###\n",
    "\n",
    "    ### The generator - should remain the same ###\n",
    "    response = await azure_client.chat.completions.create(\n",
    "        model=\"sandbox-gpt-35\", # sandbox-gpt-35\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    text = response.choices[0].message.content.strip()\n",
    "\n",
    "    ### End of the generator ###\n",
    "\n",
    "    return {\"id\": id, \"text\": text, \"concerns\": selected_concerns} # Yaml Format; Adapt to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "baaab724-5731-4261-88ba-34aae57cc7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': -1,\n",
       " 'text': 'My skin has been looking dull and uneven lately. Can you recommend a serum to help with skin discoloration from your [Pigmentclar](productline) line?',\n",
       " 'concerns': ['dark_circles', 'skin_decoloration']}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = await generate_question(-1, azure_client)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab8f1e-cf05-4e01-be92-03da26071cb0",
   "metadata": {},
   "source": [
    "### The generating body\n",
    "\n",
    "The next few functions need little to no modification. They're made in such a way to insure that the API is always questioned slowly and non-blockingly. While running initial tests in this notebook's first ever version, I had many unpleasant surprises with a little error called *RateLimitError*. This arises when you send too many requests to the API, and unless you have an unlimited plan, I suggest you keep it.\n",
    "\n",
    "I also have to mention that when this error arises, and the **call_api_with_retry** comes out of the cooldown period, the generation will start from the beginning, i.e. you could be at 99% of the way and this error will wipe everything out so you have to start over. One way of combatting this issue will be addressed in the next section, *Generating our datasets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3a84e9b-14d5-453c-b873-afb70cfefdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is vital.\n",
    "# We have a limit on how many calls we can make.\n",
    "async def call_api_with_retry(task, max_retries=5, base_delay=0.5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return await task()\n",
    "        except openai.RateLimitError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep_time = base_delay * 2 ** attempt + random.uniform(0, 0.1 * base_delay)\n",
    "                print(f\"Rate limit reached, retrying after {sleep_time:.2f} seconds...\", flush=True)\n",
    "                await asyncio.sleep(sleep_time)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\", flush=True)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03548e44-c632-4d9b-8a86-ec6a6de8a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate function\n",
    "async def generate_questions(num_entries, azure_client=azure_client):\n",
    "    dataset = []\n",
    "    for i in tqdm(range(num_entries)):\n",
    "        question = await generate_question(i + 1, azure_client)\n",
    "        dataset.append(question)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b04ca6f1-78fd-4a54-b1df-2e1aabae77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator function\n",
    "async def generate_dataset(num_entries, azure_client=azure_client):\n",
    "    return await call_api_with_retry(lambda: generate_questions(num_entries, azure_client))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52876d66-8532-466a-b73e-ccf4b295efc5",
   "metadata": {},
   "source": [
    "## Generating our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8ffb7-c1a8-4448-adb0-ace6f631f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85-15 split, adapt as needed, I use mini-batches \n",
    "batch_size = 100\n",
    "train_iter = 6000//batch_size # We're generating 6000 examples for the training dataset\n",
    "test_iter = 1000//batch_size # We're generating 1000 examples for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "10275e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [26:55<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/lrp/testtrain/data.yml\"\n",
    "batch = 600\n",
    "dataset = await generate_dataset(batch)\n",
    "with open(path, 'w') as file:\n",
    "    yaml.dump(dataset, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79981de4-fb77-4f76-9b9f-dbf29ca0f8a7",
   "metadata": {},
   "source": [
    "### Battling the *RateLimitError* (my way)\n",
    "\n",
    "As mentioned in the previous section, *RateLimitError* is your worst enemy, and I had to learn it the hard way. I've tested many different ways of dealing with it, but it would eventually always arise after ~700 generated examples in one go.\n",
    "\n",
    "So, this is one of my solutions: set up checkpoints of no more than 100 examples, and loop through it. Though 100 examples per checkpoint might seem too less, what you have to take into account is the fact that the speed at which the API communicates with the model is constant. Let's say generating 100 examples takes 5 minutes, then generating 1000 examples would take 50 minutes in any case. However, letting the model generate one file of 1000 examples will result in a *critical RateLimitError** 98% of the time (I had to learn that one the hard way as well), but generating 10 files with 100 examples each guarantees that at the end of those 50 minutes you will almost always have the result you were seeking. \n",
    "\n",
    "Keep in mind that a *RateLimitError* might still occur, you can consider it \"normal\" behaviour as many factors play into the performance of the API calls. If it does, nothing is lost as you have all of the previously generated mini-batch files saved and only the *current* file is being regenerated.\n",
    "\n",
    "I didn't include the handling of different files as that is something unique to every scenario, but what worked for me is just using the **yaml** library to merge all of the files into one with a simple loop.\n",
    "\n",
    "\n",
    "* *Critical RateLimitError*: To better understand this term, we have to go back to the **call_api_with_retry** function. This function allows for a maximum of 5 retries before it shuts down the whole process. A *critical RateLimitError* occurs when we run out of retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3508d-8756-4fc0-bed5-9706b1f0ef74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "\n",
    "for i in range():\n",
    "    train_dataset_path = f\"../data/lrp/train/lrp_train_60.yml\"\n",
    "    train_dataset = await generate_dataset(batch_size)\n",
    "\n",
    "    with open(train_dataset_path, \"w\") as file:\n",
    "        yaml.dump(train_dataset, file, sort_keys=False)\n",
    "\n",
    "print('Train dataset generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4f91c-a662-40ce-a921-98949a4fef5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "for i in range(test_iter):\n",
    "    test_dataset_path = f\"../data/lrp/test/lrp_test_{i+1}.yml\"\n",
    "    test_dataset = await generate_dataset(batch_size)\n",
    "\n",
    "    with open(test_dataset_path, \"w\") as file:\n",
    "        yaml.dump(test_dataset, file, sort_keys=False)\n",
    "\n",
    "print('Test dataset generated.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
